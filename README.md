# News Scraper API
Une API REST construite avec NestJS qui scrape et stocke des articles de diffÃ©rentes sources d'actualitÃ©s.

## FonctionnalitÃ©s

- Scraping automatique d'articles depuis Hacker News
- Mise en cache avec Redis
- Pagination, filtre de titre ainsi que de source
- Nettoyage automatique des anciens articles
- API RESTful

## ğŸ“‹ PrÃ©requis

- Node.js
- MySQL
- Redis
- pnpm

## ğŸ› ï¸ Installation

### Option 1: Installation Locale

1. **Cloner le repository**
```bash
git clone git@github.com:upnico9/news-scrapper.git
cd news-scraper
```

2. **Installer les dÃ©pendances**
```bash
pnpm install
```

3. **Configurer les variables d'environnement**
```bash
cp .env.example .env
```

Configurer le fichier `.env` :
```env
# Database
DB_HOST=localhost
DB_PORT=3306
DB_USERNAME=root
DB_PASSWORD=your_password
DB_DATABASE=news

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
```

4. **DÃ©marrer les services**
```bash
# MySQL
sudo service mysql start

# Redis
sudo service redis-server start
```

5. **Initialiser la base de donnÃ©es**
```bash
pnpm run db:init
```

6. **Lancer l'application**
```bash
pnpm run start:dev
```

### Option 2: Installation avec Docker

1. **Cloner le repository**
```bash
git clone git@github.com:upnico9/news-scrapper.git
cd news-scraper
```

2. **Configurer les variables d'environnement (optionnel)**
Les variables d'environnement sont dÃ©jÃ  configurÃ©es dans le fichier `docker-compose.yml`. Vous pouvez les modifier si nÃ©cessaire.

3. **Construire et dÃ©marrer les conteneurs**
```bash
docker compose up --build
```

4. **AccÃ©der au swagger**
Le swagger sera disponible Ã  l'adresse: http://localhost:3000/api

5. **ArrÃªter les conteneurs**
```bash
docker compose down
```

Pour supprimer Ã©galement les volumes (donnÃ©es persistantes):
```bash
docker compose down -v
```

## ğŸ“š API Documentation


## Architecture

```
src/
â”œâ”€â”€ articles/           # Gestion des articles
â”‚   â”œâ”€â”€ dto/           # Data Transfer Objects
â”‚   â”œâ”€â”€ entities/      # EntitÃ©s de base de donnÃ©es
â”‚
â”œâ”€â”€ scraper/           # Service de scraping
â””â”€â”€ database/          # Configuration DB
```

## TÃ¢ches automatisÃ©es

| TÃ¢che | FrÃ©quence |
|-------|-----------|
| Scraping des articles | Toutes les 10 minutes |
| Nettoyage des articles | Tous les jours Ã  minuit |

## ProblÃ¨mes IdentifiÃ©s sur un plus gros scrapping 

### 1. Performance des RequÃªtes

**ProblÃ¨me** : Les requÃªtes de rÃ©cupÃ©ration d'articles peuvent devenir lentes avec l'augmentation du volume de donnÃ©es.

**Solutions** :
- Mise en place du cache Redis pour les requÃªtes frÃ©quentes
- Pagination des rÃ©sultats
- Indexation de la base de donnÃ©es sur les colonnes frÃ©quemment utilisÃ©es (titre, source, date de publication)
- Nettoyage automatique des articles de plus de 30 jours
- Archivage des anciens articles dans une table sÃ©parÃ©e (?)
- Sharding des bases de donnÃ©es (?)


## Solution ImplÃ©mentÃ©e

1. **Cache Redis**
   - RÃ©duit la charge sur la base de donnÃ©es
   - AmÃ©liore le temps de rÃ©ponse pour les requÃªtes frÃ©quentes
   - TTL de 5 minutes pour garantir la fraÃ®cheur des donnÃ©es

2. **Nettoyage Automatique**
   - Supprime les articles de plus de 30 jours
   - Maintient la performance de la base de donnÃ©es
   - ExÃ©cution quotidienne Ã  minuit



## Conclusion

Toujours dans une dÃ©marche de m'amÃ©liorer je serai ravi de discuter de mes difficultÃ©s, des choses que j'aurai pu mieux faire.

Une collection POSTMAN est glissÃ©e dans le rÃ©pository.

Un swagger est disponible au lancement du serveur directement sur host /api


Bonne lecture :)